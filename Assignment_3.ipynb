{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiXro3Tfp0omefqtgQJhmj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khushisingare/Portfolio/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IJ2Tfo3MY0Zt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def entropy(y):\n",
        "    \"\"\"\n",
        "    Calculating the entropy of a list of labels.\n",
        "    \"\"\"\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    prob = counts / len(y)\n",
        "    entropy = -np.sum(prob * np.log2(prob))\n",
        "    return entropy\n",
        "def information_gain(X, y, feature_idx):\n",
        "    \"\"\"\n",
        "    Calculating the information gain of a feature.\n",
        "    \"\"\"\n",
        "    total_entropy = entropy(y)\n",
        "    values, counts = np.unique(X[:, feature_idx], return_counts=True)\n",
        "    weighted_entropy = 0\n",
        "    for value, count in zip(values, counts):\n",
        "        subset_entropy = entropy(y[X[:, feature_idx] == value])\n",
        "        weighted_entropy += count / len(X) * subset_entropy\n",
        "    information_gain = total_entropy - weighted_entropy\n",
        "    return information_gain\n",
        "def find_best_split(X, y):\n",
        "    \"\"\"\n",
        "    Finding the best feature to split on based on information gain.\n",
        "    \"\"\"\n",
        "    best_gain = -1\n",
        "    best_feature = None\n",
        "    for feature_idx in range(X.shape[1]):\n",
        "        gain = information_gain(X, y, feature_idx)\n",
        "        if gain > best_gain:\n",
        "          best_gain = gain\n",
        "          best_feature = feature_idx\n",
        "    return best_feature\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree_ = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
        "            return np.argmax(np.bincount(y))\n",
        "\n",
        "        feature_idx = find_best_split(X, y)\n",
        "        if feature_idx is None:\n",
        "            return np.argmax(np.bincount(y))\n",
        "\n",
        "        values = np.unique(X[:, feature_idx])\n",
        "        node = {}\n",
        "        node[feature_idx] = {}\n",
        "        for value in values:\n",
        "            mask = X[:, feature_idx] == value\n",
        "            node[feature_idx][value] = self._grow_tree(X[mask], y[mask], depth + 1)\n",
        "        return node\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(inputs) for inputs in X])\n",
        "\n",
        "    def _predict(self, inputs):\n",
        "        node = self.tree_\n",
        "        while isinstance(node, dict):\n",
        "            feature_idx = list(node.keys())[0]\n",
        "            node = node[feature_idx][inputs[feature_idx]]\n",
        "        return node\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])\n"
      ]
    }
  ]
}